{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9103e92c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (8.3.167)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from ultralytics) (4.64.1)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from ultralytics) (1.15.3)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from ultralytics) (6.0)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from ultralytics) (4.12.0.88)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from ultralytics) (2.0.14)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from ultralytics) (9.4.0)\n",
      "Requirement already satisfied: numpy>=1.23.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from ultralytics) (2.2.6)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from ultralytics) (2.5.1+cu121)\n",
      "Requirement already satisfied: psutil in c:\\users\\dell\\anaconda3\\lib\\site-packages (from ultralytics) (5.9.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from ultralytics) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from ultralytics) (2.28.1)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from ultralytics) (0.20.1+cu121)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from ultralytics) (3.7.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\dell\\anaconda3\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.0.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (22.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2022.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2025.7.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (1.26.20)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\dell\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2022.11.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\dell\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.14.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\dell\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2.8.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "#Install muna ito\n",
    "!pip install ultralytics --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c46e0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version used by PyTorch: 12.1\n",
      "Number of GPUs: 1\n",
      "Current GPU: Quadro P1000\n",
      "\n",
      "Tensor on CPU: tensor([[0.5607, 0.0505, 0.3810],\n",
      "        [0.8048, 0.8557, 0.3600],\n",
      "        [0.7179, 0.6079, 0.7426],\n",
      "        [0.8717, 0.4399, 0.4379],\n",
      "        [0.6897, 0.3079, 0.3350]])\n",
      "Tensor on GPU: tensor([[0.5607, 0.0505, 0.3810],\n",
      "        [0.8048, 0.8557, 0.3600],\n",
      "        [0.7179, 0.6079, 0.7426],\n",
      "        [0.8717, 0.4399, 0.4379],\n",
      "        [0.6897, 0.3079, 0.3350]], device='cuda:0')\n",
      "Is tensor on CUDA? True\n"
     ]
    }
   ],
   "source": [
    "#then ito\n",
    "#dapat nvidia gpu and version 12.9 ang cuda\n",
    "#adjust code kung iba version ng cuda\n",
    "#kapag hindi nvidia gpu, i-gpt na lang kung paano ilipat from GPU to CPU\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version used by PyTorch: {torch.version.cuda}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Test CUDA\n",
    "    x = torch.rand(5, 3)\n",
    "    print(f\"\\nTensor on CPU: {x}\")\n",
    "    \n",
    "    x = x.cuda()\n",
    "    print(f\"Tensor on GPU: {x}\")\n",
    "    print(f\"Is tensor on CUDA? {x.is_cuda}\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 4,
>>>>>>> 57d68d73214ebd7cb640120d0a856c49926a961f
   "id": "6baa07ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model found at: C:\\Users\\DELL\\Downloads\\Zero Index Capstone Project Front End\\assets\\model\\best.pt\n",
      "File size: 21.47 MB\n"
     ]
    }
   ],
   "source": [
    "#kapag okay na sa setup and install, I-run muna ito para maload ang model\n",
    "import os\n",
    "\n",
    "# Your model path\n",
    "MODEL_PATH = r\"C:\\Users\\DELL\\Downloads\\Zero Index Capstone Project Front End\\assets\\model\\best.pt\"\n",
    "\n",
    "# Verify the file exists\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(f\"✓ Model found at: {MODEL_PATH}\")\n",
    "    print(f\"File size: {os.path.getsize(MODEL_PATH) / 1024 / 1024:.2f} MB\")\n",
    "else:\n",
    "    print(f\"✗ Model not found at: {MODEL_PATH}\")\n",
    "    print(\"Please check the path and try again\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
   "id": "0837ee09",
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 7,
   "id": "0837ee09",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to configure default ndarray.__repr__",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\_core\\arrayprint.py:34\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m numerictypes \u001b[38;5;28;01mas\u001b[39;00m _nt\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mumath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m absolute, isinf, isfinite, isnat\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\_core\\numerictypes.py:102\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_string_helpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     english_lower, english_upper, english_capitalize, LOWER_TABLE, UPPER_TABLE\n\u001b[0;32m    100\u001b[0m )\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_type_aliases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    103\u001b[0m     sctypeDict, allTypes, sctypes\n\u001b[0;32m    104\u001b[0m )\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dtype\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _kind_name\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\_core\\_type_aliases.py:38\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _abstract_type_name \u001b[38;5;129;01min\u001b[39;00m _abstract_type_names:\n\u001b[1;32m---> 38\u001b[0m     allTypes[_abstract_type_name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_abstract_type_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m typeinfo\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy.core.multiarray' has no attribute 'inexact'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m     sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(detect_path)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Import from detect.py\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdetect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m zone_definitions\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Your model path\u001b[39;00m\n\u001b[0;32m     22\u001b[0m MODEL_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDELL\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mZero Index Capstone Project Front End\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124massets\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbest.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\Downloads\\Zero Index Capstone Project Front End\\detect.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mshapely\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeometry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Polygon, Point\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msort\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sort\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "File \u001b[1;32m~\\Downloads\\Zero Index Capstone Project Front End\\sort.py:26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatches\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpatches\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m io\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\skimage\\__init__.py:157\u001b[0m\n\u001b[0;32m    154\u001b[0m         _raise_build_error(e)\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# All skimage root imports go here\u001b[39;00m\n\u001b[1;32m--> 157\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtype\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (img_as_float32,\n\u001b[0;32m    158\u001b[0m                              img_as_float64,\n\u001b[0;32m    159\u001b[0m                              img_as_float,\n\u001b[0;32m    160\u001b[0m                              img_as_int,\n\u001b[0;32m    161\u001b[0m                              img_as_uint,\n\u001b[0;32m    162\u001b[0m                              img_as_ubyte,\n\u001b[0;32m    163\u001b[0m                              img_as_bool,\n\u001b[0;32m    164\u001b[0m                              dtype_limits)\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlookfor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lookfor\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m __version__:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# Append last commit date and hash to dev version information, if available\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\skimage\\util\\__init__.py:16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munique\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unique_rows\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_invert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m invert\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_montage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m montage\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_map_array\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m map_array\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_label\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m label_points\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\skimage\\util\\_montage.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_shared\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m exposure\n\u001b[0;32m      6\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmontage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;129m@utils\u001b[39m\u001b[38;5;241m.\u001b[39mchannel_as_last_axis(multichannel_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;129m@utils\u001b[39m\u001b[38;5;241m.\u001b[39mdeprecate_multichannel_kwarg()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmontage\u001b[39m(arr_in, fill\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m, rescale_intensity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, grid_shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     12\u001b[0m             padding_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, multichannel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, channel_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\skimage\\_shared\\lazy.py:62\u001b[0m, in \u001b[0;36mattach.<locals>.__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(name):\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m submodules:\n\u001b[1;32m---> 62\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpackage_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m attr_to_modules:\n\u001b[0;32m     64\u001b[0m         submod \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\n\u001b[0;32m     65\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_to_modules[name]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     66\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\skimage\\exposure\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexposure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m histogram, equalize_hist, \\\n\u001b[0;32m      2\u001b[0m                       rescale_intensity, cumulative_distribution, \\\n\u001b[0;32m      3\u001b[0m                       adjust_gamma, adjust_sigmoid, adjust_log, \\\n\u001b[0;32m      4\u001b[0m                       is_low_contrast\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_adapthist\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m equalize_adapthist\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhistogram_matching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m match_histograms\n\u001b[0;32m     10\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhistogram\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     11\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mequalize_hist\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     12\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mequalize_adapthist\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_low_contrast\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     19\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatch_histograms\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\skimage\\exposure\\_adapthist.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_shared\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _supported_float_type\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madapt_rgb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m adapt_rgb, hsv_value\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexposure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rescale_intensity\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m img_as_uint\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\skimage\\color\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolorconv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (convert_colorspace,\n\u001b[0;32m      2\u001b[0m                         rgba2rgb,\n\u001b[0;32m      3\u001b[0m                         rgb2hsv,\n\u001b[0;32m      4\u001b[0m                         hsv2rgb,\n\u001b[0;32m      5\u001b[0m                         rgb2xyz,\n\u001b[0;32m      6\u001b[0m                         xyz2rgb,\n\u001b[0;32m      7\u001b[0m                         rgb2rgbcie,\n\u001b[0;32m      8\u001b[0m                         rgbcie2rgb,\n\u001b[0;32m      9\u001b[0m                         rgb2gray,\n\u001b[0;32m     10\u001b[0m                         gray2rgb,\n\u001b[0;32m     11\u001b[0m                         gray2rgba,\n\u001b[0;32m     12\u001b[0m                         xyz2lab,\n\u001b[0;32m     13\u001b[0m                         lab2xyz,\n\u001b[0;32m     14\u001b[0m                         lab2rgb,\n\u001b[0;32m     15\u001b[0m                         rgb2lab,\n\u001b[0;32m     16\u001b[0m                         xyz2luv,\n\u001b[0;32m     17\u001b[0m                         luv2xyz,\n\u001b[0;32m     18\u001b[0m                         luv2rgb,\n\u001b[0;32m     19\u001b[0m                         rgb2luv,\n\u001b[0;32m     20\u001b[0m                         rgb2hed,\n\u001b[0;32m     21\u001b[0m                         hed2rgb,\n\u001b[0;32m     22\u001b[0m                         lab2lch,\n\u001b[0;32m     23\u001b[0m                         lch2lab,\n\u001b[0;32m     24\u001b[0m                         rgb2yuv,\n\u001b[0;32m     25\u001b[0m                         yuv2rgb,\n\u001b[0;32m     26\u001b[0m                         rgb2yiq,\n\u001b[0;32m     27\u001b[0m                         yiq2rgb,\n\u001b[0;32m     28\u001b[0m                         rgb2ypbpr,\n\u001b[0;32m     29\u001b[0m                         ypbpr2rgb,\n\u001b[0;32m     30\u001b[0m                         rgb2ycbcr,\n\u001b[0;32m     31\u001b[0m                         ycbcr2rgb,\n\u001b[0;32m     32\u001b[0m                         rgb2ydbdr,\n\u001b[0;32m     33\u001b[0m                         ydbdr2rgb,\n\u001b[0;32m     34\u001b[0m                         separate_stains,\n\u001b[0;32m     35\u001b[0m                         combine_stains,\n\u001b[0;32m     36\u001b[0m                         rgb_from_hed,\n\u001b[0;32m     37\u001b[0m                         hed_from_rgb,\n\u001b[0;32m     38\u001b[0m                         rgb_from_hdx,\n\u001b[0;32m     39\u001b[0m                         hdx_from_rgb,\n\u001b[0;32m     40\u001b[0m                         rgb_from_fgx,\n\u001b[0;32m     41\u001b[0m                         fgx_from_rgb,\n\u001b[0;32m     42\u001b[0m                         rgb_from_bex,\n\u001b[0;32m     43\u001b[0m                         bex_from_rgb,\n\u001b[0;32m     44\u001b[0m                         rgb_from_rbd,\n\u001b[0;32m     45\u001b[0m                         rbd_from_rgb,\n\u001b[0;32m     46\u001b[0m                         rgb_from_gdx,\n\u001b[0;32m     47\u001b[0m                         gdx_from_rgb,\n\u001b[0;32m     48\u001b[0m                         rgb_from_hax,\n\u001b[0;32m     49\u001b[0m                         hax_from_rgb,\n\u001b[0;32m     50\u001b[0m                         rgb_from_bro,\n\u001b[0;32m     51\u001b[0m                         bro_from_rgb,\n\u001b[0;32m     52\u001b[0m                         rgb_from_bpx,\n\u001b[0;32m     53\u001b[0m                         bpx_from_rgb,\n\u001b[0;32m     54\u001b[0m                         rgb_from_ahx,\n\u001b[0;32m     55\u001b[0m                         ahx_from_rgb,\n\u001b[0;32m     56\u001b[0m                         rgb_from_hpx,\n\u001b[0;32m     57\u001b[0m                         hpx_from_rgb)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolorlabel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m color_dict, label2rgb\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdelta_e\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (deltaE_cie76,\n\u001b[0;32m     62\u001b[0m                       deltaE_ciede94,\n\u001b[0;32m     63\u001b[0m                       deltaE_ciede2000,\n\u001b[0;32m     64\u001b[0m                       deltaE_cmc,\n\u001b[0;32m     65\u001b[0m                       )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\skimage\\color\\colorconv.py:407\u001b[0m\n\u001b[0;32m    404\u001b[0m rgbcie_from_xyz \u001b[38;5;241m=\u001b[39m linalg\u001b[38;5;241m.\u001b[39minv(xyz_from_rgbcie)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;66;03m# construct matrices to and from rgb:\u001b[39;00m\n\u001b[1;32m--> 407\u001b[0m rgbcie_from_rgb \u001b[38;5;241m=\u001b[39m \u001b[43mrgbcie_from_xyz\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mxyz_from_rgb\u001b[49m\n\u001b[0;32m    408\u001b[0m rgb_from_rgbcie \u001b[38;5;241m=\u001b[39m rgb_from_xyz \u001b[38;5;241m@\u001b[39m xyz_from_rgbcie\n\u001b[0;32m    411\u001b[0m gray_from_rgb \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m0.2125\u001b[39m, \u001b[38;5;241m0.7154\u001b[39m, \u001b[38;5;241m0.0721\u001b[39m],\n\u001b[0;32m    412\u001b[0m                           [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    413\u001b[0m                           [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]])\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_internal.py:855\u001b[0m, in \u001b[0;36marray_ufunc_errmsg_formatter\u001b[1;34m(dummy, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    853\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21marray_ufunc_errmsg_formatter\u001b[39m(dummy, ufunc, method, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;124;03m\"\"\" Format the error message for when __array_ufunc__ gives up. \"\"\"\u001b[39;00m\n\u001b[1;32m--> 855\u001b[0m     args_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m inputs] \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    856\u001b[0m                             [\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k, v)\n\u001b[0;32m    857\u001b[0m                              \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()])\n\u001b[0;32m    858\u001b[0m     args \u001b[38;5;241m=\u001b[39m inputs \u001b[38;5;241m+\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m, ())\n\u001b[0;32m    859\u001b[0m     types_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(arg)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_internal.py:855\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    853\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21marray_ufunc_errmsg_formatter\u001b[39m(dummy, ufunc, method, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;124;03m\"\"\" Format the error message for when __array_ufunc__ gives up. \"\"\"\u001b[39;00m\n\u001b[1;32m--> 855\u001b[0m     args_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{!r}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m inputs] \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    856\u001b[0m                             [\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k, v)\n\u001b[0;32m    857\u001b[0m                              \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()])\n\u001b[0;32m    858\u001b[0m     args \u001b[38;5;241m=\u001b[39m inputs \u001b[38;5;241m+\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m, ())\n\u001b[0;32m    859\u001b[0m     types_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(arg)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unable to configure default ndarray.__repr__"
     ]
    }
   ],
>>>>>>> 57d68d73214ebd7cb640120d0a856c49926a961f
   "source": [
    "#FLASK SETUP WITH DETECT.PY RUN_DETECT LOGIC\n",
    "#Run this cell before startin the server.\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from flask_cors import CORS\n",
    "from flask import Flask, request, jsonify\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "from shapely.geometry import Polygon, Point\n",
    "\n",
    "# Add the path where detect.py is located\n",
    "detect_path = r\"C:\\Users\\DELL\\Downloads\\Zero Index Capstone Project Front End\"\n",
    "if detect_path not in sys.path:\n",
    "    sys.path.append(detect_path)\n",
    "\n",
    "# Import from detect.py\n",
    "from detect import zone_definitions\n",
    "\n",
    "# Your model path\n",
    "MODEL_PATH = r\"C:\\Users\\DELL\\Downloads\\Zero Index Capstone Project Front End\\assets\\model\\best.pt\"\n",
    "\n",
    "class SubwayDetectionServer:\n",
    "    def __init__(self, model_path):\n",
    "        self.app = Flask(__name__)\n",
    "        CORS(self.app)\n",
    "        self.model_path = model_path\n",
    "        self.model = None\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        # Use zones from detect.py\n",
    "        self.zone_definitions = zone_definitions\n",
    "        \n",
    "        # Statistics\n",
    "        self.stats = {\n",
    "            \"total_detections\": 0,\n",
    "            \"danger_breaches\": 0,\n",
    "            \"warning_breaches\": 0,\n",
    "            \"breach_logs\": []\n",
    "        }\n",
    "        \n",
    "        # Print system info\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        if self.device == 'cuda':\n",
    "            print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"CUDA version: {torch.version.cuda}\")\n",
    "        \n",
    "        # Load model\n",
    "        self.load_model()\n",
    "        \n",
    "        # Setup routes\n",
    "        self.setup_routes()\n",
    "    \n",
    "    def load_model(self):\n",
    "        if not os.path.exists(self.model_path):\n",
    "            raise FileNotFoundError(f\"Model not found at: {self.model_path}\")\n",
    "        \n",
    "        try:\n",
    "            print(f\"Loading model from: {self.model_path}\")\n",
    "            self.model = YOLO(self.model_path)\n",
    "            self.model.to(self.device)\n",
    "            \n",
    "            # Warm up the model\n",
    "            dummy_img = np.zeros((640, 640, 3), dtype=np.uint8)\n",
    "            with torch.no_grad():\n",
    "                _ = self.model(dummy_img)\n",
    "            \n",
    "            print(f\"✓ Model loaded successfully on {self.device}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading model: {e}\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "    \n",
    "    def check_zone_breach_foot(self, x1, y1, x2, y2, camera_name=\"cam1\"):\n",
    "        \"\"\"Check if a person's foot position is in danger or warning zone\"\"\"\n",
    "        zones = self.zone_definitions.get(camera_name, self.zone_definitions[\"cam1\"])\n",
    "        \n",
    "        # Calculate foot position (bottom center of bounding box)\n",
    "        foot_x = int((x1 + x2) / 2)\n",
    "        foot_y = int(y2)  # Bottom of the bounding box\n",
    "        \n",
    "        # Create polygons\n",
    "        point = Point(foot_x, foot_y)\n",
    "        danger_polygon = Polygon(zones[\"danger\"])\n",
    "        warning_polygon = Polygon(zones[\"warning\"])\n",
    "        \n",
    "        breach_type = None\n",
    "        if danger_polygon.contains(point):\n",
    "            breach_type = \"danger\"\n",
    "        elif warning_polygon.contains(point):\n",
    "            breach_type = \"warning\"\n",
    "        \n",
    "        return breach_type, (foot_x, foot_y)\n",
    "    \n",
    "    def process_frame(self, image, camera_name=\"cam1\"):\n",
    "        \"\"\"Process a single frame using the detection logic from detect.py\"\"\"\n",
    "        zones = self.zone_definitions.get(camera_name, self.zone_definitions[\"cam1\"])\n",
    "        zone_polygons = {z: Polygon(pts) for z, pts in zones.items()}\n",
    "        \n",
    "        # Run model prediction\n",
    "        results = self.model.predict(source=image, conf=0.5, save=False)\n",
    "        \n",
    "        detections = []\n",
    "        frame_passengers = 0\n",
    "        \n",
    "        for result in results:\n",
    "            if result.boxes is None:\n",
    "                continue\n",
    "                \n",
    "            boxes = result.boxes.xyxy.cpu().numpy()\n",
    "            confidences = result.boxes.conf.cpu().numpy()\n",
    "            classes = result.boxes.cls.cpu().numpy().astype(int)\n",
    "            \n",
    "            for box, conf, cls in zip(boxes, confidences, classes):\n",
    "                # Only process person class (class 0)\n",
    "                if cls != 0:\n",
    "                    continue\n",
    "                \n",
    "                x1, y1, x2, y2 = map(int, box[:4])\n",
    "                \n",
    "                # Use foot approximation (bottom center of bounding box)\n",
    "                foot_x = int((x1 + x2) / 2)\n",
    "                foot_y = int(y2)\n",
    "                foot_point = Point(foot_x, foot_y)\n",
    "                \n",
    "                # Check zone breach using foot position\n",
    "                breach_type = None\n",
    "                if zone_polygons[\"danger\"].contains(foot_point):\n",
    "                    breach_type = \"danger\"\n",
    "                    self.stats[\"danger_breaches\"] += 1\n",
    "                elif zone_polygons[\"warning\"].contains(foot_point):\n",
    "                    breach_type = \"warning\"\n",
    "                    self.stats[\"warning_breaches\"] += 1\n",
    "                \n",
    "                frame_passengers += 1\n",
    "                self.stats[\"total_detections\"] += 1\n",
    "                \n",
    "                # Create detection object\n",
    "                detection = {\n",
    "                    \"x1\": x1,\n",
    "                    \"y1\": y1,\n",
    "                    \"x2\": x2,\n",
    "                    \"y2\": y2,\n",
    "                    \"confidence\": float(conf),\n",
    "                    \"class\": int(cls),\n",
    "                    \"label\": \"person\",\n",
    "                    \"breach_type\": breach_type,\n",
    "                    \"center\": [int((x1 + x2) / 2), int((y1 + y2) / 2)],\n",
    "                    \"foot_position\": [foot_x, foot_y]  # Add foot position\n",
    "                }\n",
    "                detections.append(detection)\n",
    "                \n",
    "                # Log breach if detected\n",
    "                if breach_type:\n",
    "                    self.stats[\"breach_logs\"].append({\n",
    "                        \"timestamp\": datetime.now().isoformat(),\n",
    "                        \"zone\": breach_type,\n",
    "                        \"position\": [foot_x, foot_y],\n",
    "                        \"camera\": camera_name\n",
    "                    })\n",
    "        \n",
    "        return detections, frame_passengers\n",
    "    \n",
    "    def setup_routes(self):\n",
    "        @self.app.route('/health', methods=['GET'])\n",
    "        def health():\n",
    "            gpu_info = {}\n",
    "            if self.device == 'cuda':\n",
    "                gpu_info = {\n",
    "                    \"gpu_name\": torch.cuda.get_device_name(0),\n",
    "                    \"memory_allocated\": f\"{torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\",\n",
    "                    \"memory_reserved\": f\"{torch.cuda.memory_reserved(0) / 1024**2:.2f} MB\",\n",
    "                    \"cuda_version\": torch.version.cuda\n",
    "                }\n",
    "            \n",
    "            return jsonify({\n",
    "                \"status\": \"running\",\n",
    "                \"model_loaded\": self.model is not None,\n",
    "                \"device\": self.device,\n",
    "                \"gpu_info\": gpu_info,\n",
    "                \"model_path\": self.model_path,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            })\n",
    "        \n",
    "        @self.app.route('/detect', methods=['POST'])\n",
    "        def detect():\n",
    "            if self.model is None:\n",
    "                return jsonify({\"success\": False, \"error\": \"Model not loaded\"}), 500\n",
    "            \n",
    "            try:\n",
    "                file = request.files.get('image')\n",
    "                camera_name = request.form.get('camera', 'cam1')\n",
    "                \n",
    "                if not file:\n",
    "                    return jsonify({\"success\": False, \"error\": \"No image provided\"}), 400\n",
    "                \n",
    "                # Read and decode image\n",
    "                image_bytes = file.read()\n",
    "                nparr = np.frombuffer(image_bytes, np.uint8)\n",
    "                image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "                \n",
    "                if image is None:\n",
    "                    return jsonify({\"success\": False, \"error\": \"Invalid image\"}), 400\n",
    "                \n",
    "                # Process frame using the logic from detect.py\n",
    "                with torch.cuda.amp.autocast(enabled=(self.device == 'cuda')):\n",
    "                    with torch.no_grad():\n",
    "                        detections, passenger_count = self.process_frame(image, camera_name)\n",
    "                \n",
    "                return jsonify({\n",
    "                    \"success\": True,\n",
    "                    \"detections\": detections,\n",
    "                    \"count\": len(detections),\n",
    "                    \"device\": self.device,\n",
    "                    \"camera\": camera_name\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Detection error: {e}\")\n",
    "                traceback.print_exc()\n",
    "                return jsonify({\"success\": False, \"error\": str(e)}), 500\n",
    "        \n",
    "        @self.app.route('/zones/<camera_name>', methods=['GET'])\n",
    "        def get_zones(camera_name):\n",
    "            \"\"\"Get zone definitions for a specific camera\"\"\"\n",
    "            zones = self.zone_definitions.get(camera_name)\n",
    "            if zones:\n",
    "                return jsonify({\n",
    "                    \"success\": True,\n",
    "                    \"camera\": camera_name,\n",
    "                    \"zones\": zones\n",
    "                })\n",
    "            else:\n",
    "                return jsonify({\n",
    "                    \"success\": False,\n",
    "                    \"error\": f\"No zones defined for camera: {camera_name}\"\n",
    "                }), 404\n",
    "        \n",
    "        @self.app.route('/stats', methods=['GET'])\n",
    "        def get_stats():\n",
    "            return jsonify({\n",
    "                \"total_passengers\": self.stats[\"total_detections\"],\n",
    "                \"breach_counts\": {\n",
    "                    \"danger\": self.stats[\"danger_breaches\"],\n",
    "                    \"warning\": self.stats[\"warning_breaches\"]\n",
    "                },\n",
    "                \"breach_logs\": self.stats[\"breach_logs\"][-10:]  # Return last 10 breach logs\n",
    "            })\n",
    "        \n",
    "        @self.app.route('/stats/reset', methods=['POST'])\n",
    "        def reset_stats():\n",
    "            self.stats = {\n",
    "                \"total_detections\": 0,\n",
    "                \"danger_breaches\": 0,\n",
    "                \"warning_breaches\": 0,\n",
    "                \"breach_logs\": []\n",
    "            }\n",
    "            return jsonify({\"success\": True, \"message\": \"Statistics reset\"})\n",
    "        \n",
    "        @self.app.route('/log_breach', methods=['POST'])\n",
    "        def log_breach():\n",
    "            \"\"\"Log a breach event from the frontend\"\"\"\n",
    "            try:\n",
    "                data = request.json\n",
    "                breach_type = data.get('type')\n",
    "                location = data.get('location')\n",
    "                timestamp = data.get('timestamp', datetime.now().isoformat())\n",
    "                \n",
    "                # Map frontend breach types to our internal types\n",
    "                if breach_type == 'yellow_line':\n",
    "                    internal_type = 'warning'\n",
    "                elif breach_type == 'platform_edge':\n",
    "                    internal_type = 'danger'\n",
    "                else:\n",
    "                    internal_type = breach_type\n",
    "                \n",
    "                breach_entry = {\n",
    "                    \"timestamp\": timestamp,\n",
    "                    \"zone\": internal_type,\n",
    "                    \"location\": location,\n",
    "                    \"camera\": data.get('camera', 'unknown')\n",
    "                }\n",
    "                \n",
    "                self.stats[\"breach_logs\"].append(breach_entry)\n",
    "                \n",
    "                # Keep only last 100 entries to prevent memory issues\n",
    "                if len(self.stats[\"breach_logs\"]) > 100:\n",
    "                    self.stats[\"breach_logs\"] = self.stats[\"breach_logs\"][-100:]\n",
    "                \n",
    "                return jsonify({\"success\": True, \"message\": \"Breach logged\"})\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error logging breach: {e}\")\n",
    "                return jsonify({\"success\": False, \"error\": str(e)}), 500\n",
    "        \n",
    "        @self.app.route('/process_video', methods=['POST'])\n",
    "        def process_video():\n",
    "            \"\"\"Process an entire video file (similar to run_detection in detect.py)\"\"\"\n",
    "            try:\n",
    "                file = request.files.get('video')\n",
    "                camera_name = request.form.get('camera', 'cam1')\n",
    "                save_annotated = request.form.get('save_annotated', 'true').lower() == 'true'\n",
    "                \n",
    "                if not file:\n",
    "                    return jsonify({\"success\": False, \"error\": \"No video provided\"}), 400\n",
    "                \n",
    "                # Save uploaded video temporarily\n",
    "                temp_video_path = os.path.join('temp', 'uploaded_video.mp4')\n",
    "                os.makedirs('temp', exist_ok=True)\n",
    "                file.save(temp_video_path)\n",
    "                \n",
    "                # Process video\n",
    "                result = self.process_video_file(temp_video_path, camera_name, save_annotated)\n",
    "                \n",
    "                # Clean up temp file\n",
    "                os.remove(temp_video_path)\n",
    "                \n",
    "                return jsonify({\n",
    "                    \"success\": True,\n",
    "                    \"result\": result\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Video processing error: {e}\")\n",
    "                traceback.print_exc()\n",
    "                return jsonify({\"success\": False, \"error\": str(e)}), 500\n",
    "    \n",
    "    def process_video_file(self, video_path, camera_name=\"cam1\", save_annotated=True):\n",
    "        \"\"\"Process a video file using the same logic as run_detection in detect.py\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS) or 30\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        \n",
    "        zones = self.zone_definitions[camera_name]\n",
    "        zone_polygons = {z: Polygon(pts) for z, pts in zones.items()}\n",
    "        \n",
    "        output_path = None\n",
    "        if save_annotated:\n",
    "            output_dir = \"annotated\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            output_path = os.path.join(output_dir, os.path.basename(video_path))\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "        \n",
    "        frame_id = 0\n",
    "        breach_log = []\n",
    "        person_total = 0\n",
    "        red_total = 0\n",
    "        yellow_total = 0\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_id += 1\n",
    "            \n",
    "            # Run model prediction\n",
    "            results = self.model.predict(source=frame, conf=0.5, save=False)\n",
    "            \n",
    "            # Draw zones\n",
    "            for zone, pts in zones.items():\n",
    "                color = (0, 255, 0) if zone == \"safe\" else (0, 255, 255) if zone == \"warning\" else (0, 0, 255)\n",
    "                pts_array = np.array(pts, np.int32)\n",
    "                cv2.polylines(frame, [pts_array], isClosed=True, color=color, thickness=2)\n",
    "            \n",
    "            for result in results:\n",
    "                if result.boxes is None:\n",
    "                    continue\n",
    "                    \n",
    "                boxes = result.boxes.xyxy.cpu().numpy()\n",
    "                classes = result.boxes.cls.cpu().numpy().astype(int)\n",
    "                \n",
    "                for box, cls in zip(boxes, classes):\n",
    "                    if cls != 0:  # Only process person class\n",
    "                        continue\n",
    "                    \n",
    "                    x1, y1, x2, y2 = map(int, box[:4])\n",
    "                    \n",
    "                    # Use foot position (bottom center of bounding box)\n",
    "                    foot_x = int((x1 + x2) / 2)\n",
    "                    foot_y = int(y2)\n",
    "                    foot_point = Point(foot_x, foot_y)\n",
    "                    \n",
    "                    breach_type = None\n",
    "                    color = (0, 255, 0)\n",
    "                    label = \"Safe\"\n",
    "                    \n",
    "                    if zone_polygons[\"danger\"].contains(foot_point):\n",
    "                        breach_type = \"Danger\"\n",
    "                        red_total += 1\n",
    "                        color = (0, 0, 255)\n",
    "                        label = \"Danger\"\n",
    "                    elif zone_polygons[\"warning\"].contains(foot_point):\n",
    "                        breach_type = \"Warning\"\n",
    "                        yellow_total += 1\n",
    "                        color = (0, 255, 255)\n",
    "                        label = \"Warning\"\n",
    "                    \n",
    "                    if breach_type:\n",
    "                        breach_log.append({\n",
    "                            \"time\": round(frame_id / fps, 2),\n",
    "                            \"zone\": breach_type,\n",
    "                            \"position\": [foot_x, foot_y]\n",
    "                        })\n",
    "                    \n",
    "                    person_total += 1\n",
    "                    \n",
    "                    # Annotate person box and label\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                    cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "                    # Draw foot position marker\n",
    "                    cv2.circle(frame, (foot_x, foot_y), 5, color, -1)\n",
    "                    # Draw line to indicate foot position\n",
    "                    cv2.line(frame, (foot_x - 10, foot_y), (foot_x + 10, foot_y), color, 2)\n",
    "            \n",
    "            if save_annotated:\n",
    "                out.write(frame)\n",
    "        \n",
    "        cap.release()\n",
    "        if save_annotated:\n",
    "            out.release()\n",
    "        \n",
    "        return {\n",
    "            \"total_passengers\": person_total,\n",
    "            \"breach_counts\": {\n",
    "                \"danger\": red_total,\n",
    "                \"warning\": yellow_total\n",
    "            },\n",
    "            \"breaches\": breach_log,\n",
    "            \"annotated_video\": output_path if save_annotated else None\n",
    "        }\n",
    "    \n",
    "    def run(self, **kwargs):\n",
    "        \"\"\"Run the Flask server\"\"\"\n",
    "        self.app.run(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c476959",
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: Quadro P1000\n",
      "CUDA version: 12.1\n",
      "Loading model from: C:\\Users\\DELL\\Downloads\\Zero Index Capstone Project Front End\\assets\\model\\best.pt\n",
      "\n",
      "0: 640x640 (no detections), 38.3ms\n",
      "Speed: 16.8ms preprocess, 38.3ms inference, 45.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "✓ Model loaded successfully on cuda\n",
      "\n",
      "==================================================\n",
      "🚀 Subway Detection Server is running!\n",
      "==================================================\n",
      "✓ Model: best.pt\n",
      "✓ Device: cuda\n",
      "✓ GPU: Quadro P1000\n",
      "\n",
      "Endpoints:\n",
      "  • Health check: http://localhost:5000/health\n",
      "  • Detection: http://localhost:5000/detect\n",
      "  • Statistics: http://localhost:5000/stats\n",
      "\n",
      "⚠️  Press 'Interrupt' button or Kernel → Interrupt to stop\n",
      "==================================================\n",
      "\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.0.103:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [17/Jul/2025 23:00:48] \"GET /health HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:00:49] \"GET /health HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:00:49] \"GET /stats HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:01:01] \"GET /stats HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:01:06] \"GET /stats HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_14508\\3273166305.py:209: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(self.device == 'cuda')):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 train, 301.0ms\n",
      "Speed: 103.5ms preprocess, 301.0ms inference, 296.2ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:10] \"POST /detect HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:01:11] \"OPTIONS /log_breach HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 train, 99.9ms\n",
      "Speed: 3.8ms preprocess, 99.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:11] \"POST /detect HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:01:11] \"POST /log_breach HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 train, 99.8ms\n",
      "Speed: 3.6ms preprocess, 99.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:11] \"POST /detect HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:01:11] \"GET /stats HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 train, 95.1ms\n",
      "Speed: 3.7ms preprocess, 95.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:11] \"POST /detect HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 train, 166.8ms\n",
      "Speed: 4.0ms preprocess, 166.8ms inference, 11.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:12] \"POST /detect HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 train, 167.5ms\n",
      "Speed: 10.4ms preprocess, 167.5ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:12] \"POST /detect HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 train, 144.6ms\n",
      "Speed: 4.2ms preprocess, 144.6ms inference, 12.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:13] \"POST /detect HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 227.1ms\n",
      "Speed: 5.4ms preprocess, 227.1ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:13] \"POST /detect HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 114.1ms\n",
      "Speed: 4.4ms preprocess, 114.1ms inference, 10.3ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:14] \"POST /detect HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 139.0ms\n",
      "Speed: 9.1ms preprocess, 139.0ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:14] \"POST /detect HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 124.5ms\n",
      "Speed: 6.7ms preprocess, 124.5ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:14] \"POST /detect HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 1 train, 114.2ms\n",
      "Speed: 7.3ms preprocess, 114.2ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:14] \"POST /detect HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 134.4ms\n",
      "Speed: 4.0ms preprocess, 134.4ms inference, 8.9ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:15] \"POST /detect HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:01:15] \"GET /stats HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 155.9ms\n",
      "Speed: 9.4ms preprocess, 155.9ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:15] \"POST /detect HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 190.2ms\n",
      "Speed: 10.0ms preprocess, 190.2ms inference, 12.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:15] \"POST /detect HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:16] \"GET /stats HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 381.3ms\n",
      "Speed: 6.1ms preprocess, 381.3ms inference, 11.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:16] \"POST /detect HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 472.4ms\n",
      "Speed: 22.4ms preprocess, 472.4ms inference, 13.2ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:17] \"POST /detect HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 290.5ms\n",
      "Speed: 8.4ms preprocess, 290.5ms inference, 17.2ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:18] \"POST /detect HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 799.1ms\n",
      "Speed: 9.0ms preprocess, 799.1ms inference, 24.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:19] \"POST /detect HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 260.5ms\n",
      "Speed: 6.0ms preprocess, 260.5ms inference, 9.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:19] \"POST /detect HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 204.9ms\n",
      "Speed: 10.6ms preprocess, 204.9ms inference, 10.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:20] \"POST /detect HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:01:20] \"GET /stats HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 167.8ms\n",
      "Speed: 4.6ms preprocess, 167.8ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:20] \"POST /detect HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:01:21] \"GET /stats HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 105.9ms\n",
      "Speed: 4.0ms preprocess, 105.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:21] \"POST /detect HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 67.7ms\n",
      "Speed: 2.4ms preprocess, 67.7ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:22] \"POST /detect HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:01:22] \"OPTIONS /log_breach HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:01:23] \"POST /log_breach HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 4 persons, 176.1ms\n",
      "Speed: 4.0ms preprocess, 176.1ms inference, 21.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:23] \"POST /detect HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 4 persons, 105.3ms\n",
      "Speed: 3.7ms preprocess, 105.3ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:24] \"POST /detect HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 4 persons, 182.3ms\n",
      "Speed: 6.0ms preprocess, 182.3ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:24] \"POST /detect HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 4 persons, 381.4ms\n",
      "Speed: 6.6ms preprocess, 381.4ms inference, 9.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:25] \"POST /detect HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:01:25] \"GET /stats HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 4 persons, 1 train, 147.9ms\n",
      "Speed: 5.1ms preprocess, 147.9ms inference, 7.1ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:25] \"POST /detect HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:01:26] \"GET /stats HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 1 train, 142.3ms\n",
      "Speed: 3.4ms preprocess, 142.3ms inference, 7.2ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:27] \"POST /detect HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 4 persons, 1 train, 97.3ms\n",
      "Speed: 3.3ms preprocess, 97.3ms inference, 7.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:27] \"POST /detect HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 4 persons, 1 train, 61.0ms\n",
      "Speed: 2.6ms preprocess, 61.0ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:28] \"POST /detect HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 4 persons, 2 trains, 114.3ms\n",
      "Speed: 3.1ms preprocess, 114.3ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:29] \"POST /detect HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:01:30] \"GET /stats HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 4 persons, 2 trains, 77.0ms\n",
      "Speed: 2.2ms preprocess, 77.0ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:30] \"POST /detect HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:01:31] \"GET /stats HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 4 persons, 2 trains, 68.2ms\n",
      "Speed: 2.2ms preprocess, 68.2ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:31] \"POST /detect HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 4 persons, 2 trains, 101.4ms\n",
      "Speed: 2.3ms preprocess, 101.4ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:32] \"POST /detect HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 4 persons, 1 train, 156.2ms\n",
      "Speed: 4.4ms preprocess, 156.2ms inference, 8.2ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:33] \"POST /detect HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 4 persons, 1 train, 113.3ms\n",
      "Speed: 5.7ms preprocess, 113.3ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:34] \"POST /detect HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:01:35] \"GET /stats HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:01:36] \"GET /stats HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 4 persons, 1 train, 63.9ms\n",
      "Speed: 2.3ms preprocess, 63.9ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:36] \"POST /detect HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 train, 88.9ms\n",
      "Speed: 2.5ms preprocess, 88.9ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:38] \"POST /detect HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:01:40] \"GET /stats HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 train, 69.9ms\n",
      "Speed: 2.4ms preprocess, 69.9ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jul/2025 23:01:40] \"POST /detect HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:01:41] \"GET /stats HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:01:45] \"GET /stats HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:01:46] \"GET /stats HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:01:50] \"GET /stats HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:01:51] \"GET /stats HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:01:55] \"GET /stats HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:01:56] \"GET /stats HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:02:00] \"GET /stats HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:02:01] \"GET /stats HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:02:05] \"GET /stats HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:02:06] \"GET /stats HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:02:10] \"GET /stats HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:02:11] \"GET /stats HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:02:15] \"GET /stats HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:02:16] \"GET /stats HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:02:20] \"GET /stats HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:02:21] \"GET /stats HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:02:49] \"GET /stats HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:02:50] \"GET /stats HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:03:49] \"GET /stats HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:03:50] \"GET /stats HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:04:49] \"GET /stats HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:04:49] \"GET /stats HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:05:49] \"GET /stats HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:05:49] \"GET /stats HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:06:49] \"GET /stats HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2025 23:06:49] \"GET /stats HTTP/1.1\" 200 -\n"
     ]
    }
   ],
=======
   "outputs": [],
>>>>>>> 57d68d73214ebd7cb640120d0a856c49926a961f
   "source": [
    "# Start the server with your model\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    try:\n",
    "        # Create and run the server\n",
    "        server = SubwayDetectionServer(MODEL_PATH)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"🚀 Subway Detection Server is running!\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"✓ Model: best.pt\")\n",
    "        print(f\"✓ Device: {server.device}\")\n",
    "        if server.device == 'cuda':\n",
    "            print(f\"✓ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"\\nEndpoints:\")\n",
    "        print(f\"  • Health check: http://localhost:5000/health\")\n",
    "        print(f\"  • Detection: http://localhost:5000/detect\")\n",
    "        print(f\"  • Statistics: http://localhost:5000/stats\")\n",
    "        print(\"\\n⚠️  Press 'Interrupt' button or Kernel → Interrupt to stop\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        # Run the server\n",
    "        server.run(debug=True, host='0.0.0.0', port=5000, use_reloader=False)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n✓ Server stopped by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error starting server: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(f\"❌ Cannot start server - model file not found at: {MODEL_PATH}\")"
   ]
<<<<<<< HEAD
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d154c043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ NumPy: 2.2.6\n",
      "✅ All imports successful.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"✅ NumPy:\", np.__version__)\n",
    "\n",
    "from skimage import io\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "print(\"✅ All imports successful.\")\n"
   ]
=======
>>>>>>> 57d68d73214ebd7cb640120d0a856c49926a961f
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
